{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a486fb",
   "metadata": {},
   "source": [
    "# Playing with Grammar in Python\n",
    "\n",
    "Suppose our goal is to have Python read a sentence and extract some content from it. The most common application is sentiment analysis, wherein Python scans over a sentence and tells us whether the sentence has a particular sentiment (e.g. \"good\" or \"bad\").\n",
    "\n",
    "For example:\n",
    "\n",
    "> \"We had an awful quarter, sales have been terrible.\"\n",
    "\n",
    "has a negative tone. Python can detect this tone by being fed a list of negative words (which would include \"awful\" and \"terrible\") and then finding those words in the example sentence. This application is fairly straight-forward; the sample code below tells us the sentence is 100% negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc788969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# example sentence\n",
    "sentence = \"We had an awful quarter, sales have been terrible.\"\n",
    "\n",
    "# example tone lists (real lists would be much longer than these)\n",
    "positive_words = [\"great\", \"tremendous\", \"amazing\"]\n",
    "negative_words = [\"awful\", \"terrible\", \"horrific\"]\n",
    "\n",
    "# tone = num. neg. words / (num. neg. words + num. pos. words)\n",
    "num_pos = len([word for word in sentence.split() if word in positive_words])\n",
    "num_neg = len([word for word in sentence.split() if word in negative_words])\n",
    "tone = num_neg / (num_neg + num_pos)\n",
    "print(tone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd909ea",
   "metadata": {},
   "source": [
    "We can go deeper than this. Python has modules that allow us to unpack the grammar of a sentence. By doing so, we can look for more specific types of content. Here, we'll consider a search for news articles that report management issued guidance.\n",
    "\n",
    "To begin, consider an obvious instance of management guidance:\n",
    "\n",
    "> \"XYZ announced that earnings will increase this year.\"\n",
    "\n",
    "No sentence could be more plain than this. XYZ, the hypothetical company in the example above, announces that earnings are expected to increase this year. Because the topic (\"earnings\") pertains to a future period (\"this year\") rather than a prior period (e.g. \"last quarter\"), the statement is forward-looking.\n",
    "\n",
    "The task for finding management issued guidance can be broken down into three parts:\n",
    "\n",
    "1. Does the sentence pertain to relevant financial information (e.g. \"earnings\")?\n",
    "2. Does the financial information pertain to a future period (e.g. \"next quarter\")?\n",
    "3. Is the forward-looking statement being made by a company representative?\n",
    "\n",
    "Let's start with task (1). Given a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685e03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'XYZ announced that earnings will increase this year.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95dad8d",
   "metadata": {},
   "source": [
    "Begin by looking for earnings-related words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45aed9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of financial words/phrases, the full list could be much longer\n",
    "earnings_words = ['earnings', 'profitability', 'dollars per share']\n",
    "\n",
    "# scan over earnings_words and check whether these words appear in the sentence of interest\n",
    "[w in sent for w in earnings_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfed10",
   "metadata": {},
   "source": [
    "Over the three words in the list `earnings_words`, the first of these (\"earnings\") appears in the sentence.\n",
    "\n",
    "Next look for forward-looking language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c052acd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, True, False]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of forward-looking words, the full list could be much longer\n",
    "forward_words = ['forecasted', 'estimated', 'will', 'expected']\n",
    "\n",
    "# scan over forward_words and check whether these words appear in the sentence of interest\n",
    "[w in sent for w in forward_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cba21b",
   "metadata": {},
   "source": [
    "Over the four words in `forward_words`, the third of these (\"will\") appears in the sentence.\n",
    "\n",
    "We must be careful that the forward-looking language is being applied to the earnings-related word, rather than elsewhere in the sentence. For example, in the sentence below, the earnings word (\"earnings\") is in a separate and independent clause from the the forward word (\"will\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53e0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sent = '''XYZ stated that although earnings had fallen last year,\n",
    "              the board remained confident in how the new CEO will manage the company.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c84366",
   "metadata": {},
   "source": [
    "To ensure that the forward-looking word and earnings-related word are connected in the sentence, the grammar of the sentence must be convered.\n",
    "\n",
    "To do this, one can run the sentence through `spaCy` to analyze the text.\n",
    "\n",
    "*Version warning: for compatibility with a module discussed later, I'm using `spacy` version 2.1.0 here.  This is an ancient (~ June 2019) version of the module that happened to erroneously ignore the `jupyter=False` flag.  This was fixed in later versions along the 2.1.x chain, as shown [here](https://github.com/explosion/spaCy/blob/v2.1.x/spacy/displacy/__init__.py#L56).  If you want to save rendered grammar maps in version 2.1.0, correct the `spacy/displacy/__init__.py` file in your `site-packages`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cabf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spaCy module\n",
    "import spacy\n",
    "\n",
    "# pass the sentence through spaCy's text-processing pipeline\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(sent)\n",
    "\n",
    "# display the grammar of the sentence\n",
    "svg = spacy.displacy.render(doc,\n",
    "               style=\"dep\", # show the dependency strcuture,\n",
    "               options={'distance':110, # make the output smaller\n",
    "                        'collapse_phrases':True}, # collapse noun phrases\n",
    "               jupyter=False) # disable Jupyter auto-rendering (return render at HTML)\n",
    "with open('assets/dependency_map.svg', 'w', encoding='utf-8') as fout:\n",
    "    fout.write(svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e743c43",
   "metadata": {},
   "source": [
    "![map](assets/dependency_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c96d9",
   "metadata": {},
   "source": [
    "All words have a part of speech (e.g. VERB, NOUN) as well as a dependency. For example, \"XYZ\" is a proper noun and is the subject (dependency type) for the verb \"announced\" (the dependency word).\n",
    "\n",
    "We can access all of this information from the `doc` object returned from `nlp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de79a150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYZ PROPN nsubj announced\n",
      "announced VERB ROOT announced\n",
      "that ADP mark increase\n",
      "earnings NOUN nsubj increase\n",
      "will VERB aux increase\n",
      "increase VERB ccomp announced\n",
      "this DET det year\n",
      "year NOUN npadvmod increase\n",
      ". PUNCT punct announced\n"
     ]
    }
   ],
   "source": [
    "for w in doc:\n",
    "    print(w.text, w.pos_, w.dep_, w.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04159ac",
   "metadata": {},
   "source": [
    "One simple way to verify that the earnings-related word and the forward-looking word are discussing the same component of a sentence is to ensure that each of the two words shares the same verb. This ignores more complicated sentence structures, and additional checks should be added in to the code.\n",
    "\n",
    "The verb for the earnings-related word is found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6e8c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earnings increase\n"
     ]
    }
   ],
   "source": [
    "e_words = [w for w in doc if w.text in earnings_words]\n",
    "\n",
    "def get_verb(w):\n",
    "    h = w\n",
    "    while True:\n",
    "        if h.pos_ == 'VERB' and h.dep_ != 'aux':\n",
    "            break\n",
    "        h = h.head\n",
    "    return h\n",
    "\n",
    "e_verbs = {w:get_verb(w) for w in e_words}\n",
    "    \n",
    "for e, v in e_verbs.items():\n",
    "    print(e.text, v.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ee856",
   "metadata": {},
   "source": [
    "The verb for the forward-looking word is similarly found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64407ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will increase\n"
     ]
    }
   ],
   "source": [
    "f_words = [w for w in doc if w.text in forward_words]\n",
    "\n",
    "f_verbs = {w:get_verb(w) for w in f_words}\n",
    "    \n",
    "for f, v in f_verbs.items():\n",
    "    print(f.text, v.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406e252",
   "metadata": {},
   "source": [
    "Because \"earnings\" (the earnings-related word) and \"will\" (the forward-looking word) share the verb \"increase\", we can understand that the forward-looking language is being used to discuss the earnings-related topic.\n",
    "\n",
    "Note that we ignored verbs with dependency \"aux\" in the above. Auxiliary verbs modify other verbs; they are not the principal verb of the subject-verb pair that we are looking for. However, auxiliary verbs are important because they help us verify forward-looking language. English does not have a formal future tense. Rather, future actions are indicated by auxiliary phrases. For instance, \"this year's earnings increase\" is in the present tense whereas \"next year's earnings will increase\". In the latter case, the verb \"increase\" is modified by the auxiliary verb \"will\". Auxiliary verbs do not always indicate a future tense; their presence is more nuanced. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fe9df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'XYZ had expected earnings to increase last year.'\n",
    "sent2 = 'XYZ expected earnings to increase next year.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf67d7",
   "metadata": {},
   "source": [
    "In `sent1`, \"had\" modifies \"expected\" to place it in the past tense. In `sent2`, the lack of a auxiliary modifier on \"expected\" leaves it in the present tense; because \"expected\" is understood to be about future events, we know that the present tense of this word discusses future events.\n",
    "\n",
    "What remains is to determine whether the forward-looking statement about an earnings-related topic is being given by management. We don't, for instance, wish to include forecasts made by analysts. To determine the speaker in the sentence, we need to find other subjects in the sentence. The word \"earnings\" in `sent` is the subject for \"increase\" whereas the noun phrase \"XYZ\" is the subject for \"announced\". These two verbs are linked together (they are causal compliments). We begin by mapping each verb to a subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc82acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYZ announced\n",
      "earnings increase\n"
     ]
    }
   ],
   "source": [
    "def get_subjMap(doc):\n",
    "    subj_map = {}\n",
    "    for s in doc.sents:\n",
    "        for w in s:\n",
    "            if w.dep_ == 'nsubj':\n",
    "                subj_map.update({w.head: w})\n",
    "                \n",
    "    return subj_map\n",
    "        \n",
    "subj_map = get_subjMap(doc)\n",
    "for v, w in subj_map.items():\n",
    "    print(w, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293bb1b",
   "metadata": {},
   "source": [
    "Then, starting at the verb we discovered earlier (and saved in `e_verbs`), we look for related subject-verb phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce26ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earnings XYZ\n"
     ]
    }
   ],
   "source": [
    "for e, v in e_verbs.items():\n",
    "    subj = subj_map[v.head]\n",
    "    print(e, subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0bd84",
   "metadata": {},
   "source": [
    "This gives confirmation that the agent doing the forecasting is XYZ.\n",
    "\n",
    "What about instances in which it is not immediately clear from the subject of the sentence what the affiliation of the speaker is? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a19f1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = '''\n",
    "XYZ announced strong results for the quarter.\n",
    "Alice Smith, CEO of XYZ, remains optimistic.\n",
    "Bob Johnson, an analyst covering XYZ pressured Smith for details on the latest earnings call.\n",
    "Smith stated that she expected earnings growth over the next year.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6e533",
   "metadata": {},
   "source": [
    "It is the last sentence that has a forecast. However, the subject doing the forcasting is \"Smith\". Absent any other context, it is unclear from that sentence alone whether \"Smith\" is affiliated with the company. Note that her affiliation is clairified two sentences earlier.\n",
    "\n",
    "Because we've expanded the text to contain multiple setences, before going any further let's define a function to check each sentence for the information we've thus far been able to extract. If the function finds a forward-looking statement about an earnings-related item, it should return:\n",
    "\n",
    "1. the earnings-related word\n",
    "2. the forward-looking word\n",
    "3. the verb corresponding to the earnings-related word\n",
    "4. the sentence\n",
    "A sentence may have multiple instances of items (1)-(3), so the function should be structured to return a list of those instances as well as item (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98547bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith stated that she expected earnings growth over the next year.\n",
      "\n",
      "\t ['earnings', 'expected', expected]\n"
     ]
    }
   ],
   "source": [
    "docp = nlp(para)\n",
    "\n",
    "def find_sentence(doc):\n",
    "    \n",
    "    return_items = {}\n",
    "    \n",
    "    for s in doc.sents:\n",
    "\n",
    "        # look for earnings-related words\n",
    "        ep_words = [w for w in s if w.text in earnings_words]\n",
    "        ep_verbs = {w:get_verb(w) for w in ep_words}\n",
    "\n",
    "        # look for forward-looking words\n",
    "        fp_words = [w for w in s if w.text in forward_words]\n",
    "        fp_verbs = {w:get_verb(w) for w in fp_words}\n",
    "\n",
    "        # verify that the forward and earnings word match\n",
    "        for e, ev in ep_verbs.items():\n",
    "            for f, fv in fp_verbs.items():\n",
    "                if ev == fv:\n",
    "                    if s not in return_items:\n",
    "                        return_items.update({s: [[e.text, f.text, ev]]})\n",
    "                    else:\n",
    "                        return_items[s].append([e.text, f.text, ev])\n",
    "\n",
    "    return return_items\n",
    "    \n",
    "found_sentences = find_sentence(docp)\n",
    "for sentence, instances in found_sentences.items():\n",
    "    print(sentence)\n",
    "    for instance in instances:\n",
    "        print('\\t', instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c67be",
   "metadata": {},
   "source": [
    "The map of subject-verb pairs in the paragraph is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d068722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYZ announced\n",
      "Smith remains\n",
      "Johnson pressured\n",
      "Smith stated\n",
      "she expected\n"
     ]
    }
   ],
   "source": [
    "subj_map = get_subjMap(docp)\n",
    "for v, w in subj_map.items():\n",
    "    print(w, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245a263",
   "metadata": {},
   "source": [
    "And so if we go looking for the speaker in our forecast sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b0bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earnings expected expected she\n"
     ]
    }
   ],
   "source": [
    "for instances in found_sentences.values():\n",
    "    for instance in instances:\n",
    "        e, f, v = instance\n",
    "        subj = subj_map[v]\n",
    "        print(e, f, v, subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98680c1a",
   "metadata": {},
   "source": [
    "We find that the speaker is simply \"she\".\n",
    "\n",
    "To figure out who the \"she\" refers to, utilize a co-reference tool. The tool is in the `neuralcoref` module and can be added to a `spacy` pipeline.\n",
    "\n",
    "(Technical note: `neuralcoref` requires `spacy==2.1.0`, though a version for `spacy 3+` is [in development](https://github.com/explosion/spaCy/pull/7264).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1154d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2055f8c2f48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neuralcoref\n",
    "\n",
    "# create a new spacy pipeline\n",
    "nlp2 = spacy.load('en_core_web_lg')\n",
    "\n",
    "# add neuralcoref to this pipeline\n",
    "neuralcoref.add_to_pipe(nlp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beb122",
   "metadata": {},
   "source": [
    "Now, when we pass the paragraph to `spacy`, the output model includes a list of coreference clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c94c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XYZ [\n",
      "XYZ, XYZ]\n",
      "Smith [Alice Smith, CEO of XYZ, Smith, Smith, she]\n"
     ]
    }
   ],
   "source": [
    "docp2 = nlp2(para)\n",
    "\n",
    "for item in docp2._.coref_clusters:\n",
    "    print(item.main, item.mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fed7a",
   "metadata": {},
   "source": [
    "The second coreference cluster shows us that the \"she\" we're interested is in the same co-reference cluster with \"Alice Smith\", indicating that the \"she\" refers to \"Alice Smith\". Also within this co-reference cluster is the phrase \"CEO of XYZ\". Given that \"XYZ\" is the company we are interested in, we can usually deduce that the \"she\" is representing XYZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ed8816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earnings expected expected she [Smith: [Alice Smith, CEO of XYZ, Smith, Smith, she]]\n"
     ]
    }
   ],
   "source": [
    "found_sentences2 = find_sentence(docp2)\n",
    "sent_list2 = find_sentence(docp2)\n",
    "subj_map2 = get_subjMap(docp2)\n",
    "\n",
    "for instances in found_sentences2.values():\n",
    "    for instance in instances:\n",
    "        e, f, v = instance       \n",
    "        subj = subj_map2[v]\n",
    "        print(e, f, v, subj, subj._.coref_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3fc64",
   "metadata": {},
   "source": [
    "Hence, will a little bit of grammar-parsing, it is possible to find reports of management issued guidance in a news article. Obviously, the English language can be far more complex than what's shown above. A fully developed text-parser will need to consider a much richer array of problems (a text-parser I built for this sort of project needed about 800 lines of Python code just to read over the document and check various grammatical constructs). However, it's nice to see what Python can do in this simplified example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
